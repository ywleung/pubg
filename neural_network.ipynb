{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlyip\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc, sys\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_V2.csv')\n",
    "\n",
    "train.drop(train[train['winPlacePerc'].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    #start_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    #end_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_train=True):\n",
    "    # fix rank points\n",
    "    df['rankPoints'] = np.where(df['rankPoints'] <= 0, 0, df['rankPoints'])\n",
    "    \n",
    "    print('adding new features...')\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    df['headshotrate'] = df['kills'] / df['headshotKills']\n",
    "    df['killStreakrate'] = df['killStreaks'] / df['kills']\n",
    "    df['healthitems'] = df['heals'] + df['boosts']\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
    "    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n",
    "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
    "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
    "    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n",
    "    df[\"skill\"] = df[\"headshotKills\"]+df[\"roadKills\"]\n",
    "    \n",
    "    df[df == np.Inf] = np.NaN\n",
    "    df[df == np.NINF] = np.NaN\n",
    "    \n",
    "    print(\"Removing Na's From DF\")\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    print(df.isnull().any().any())\n",
    "    \n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    y = None\n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "    \n",
    "    print(\"adding group mean feature...\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()    \n",
    "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
    "    else: df_out = df[['matchId','groupId']]\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "#     print(\"get group median feature\")\n",
    "#     agg = df.groupby(['matchId','groupId'])[features].agg('median')\n",
    "#     agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "#     df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "#     df_out = df_out.merge(agg_rank, suffixes=[\"_median\", \"_median_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"adding group max feature...\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"adding group min feature...\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "#     print(\"get group sum feature\")\n",
    "#     agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n",
    "#     agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "#     df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "#     df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"adding group size feature...\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"adding match mean feature...\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "#     print(\"get match sum feature\")\n",
    "#     agg = df.groupby(['matchId'])[features].agg('sum').reset_index()\n",
    "#     df_out = df_out.merge(agg, suffixes=[\"\", \"_match_sum\"], how='left', on=['matchId'])\n",
    "    \n",
    "#     print(\"get match median feature\")\n",
    "#     agg = df.groupby(['matchId'])[features].agg('median').reset_index()\n",
    "#     df_out = df_out.merge(agg, suffixes=[\"\", \"_match_median\"], how='left', on=['matchId'])\n",
    "    \n",
    "    print(\"adding match size feature...\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    \n",
    "    del df, agg, agg_rank\n",
    "    gc.collect()\n",
    "    \n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    df_out = np.array(df_out, dtype=np.float64)\n",
    "\n",
    "#     del df, df_out, agg, agg_rank\n",
    "#     gc.collect()\n",
    "\n",
    "    return df_out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new features...\n",
      "Removing Na's From DF\n",
      "False\n",
      "adding group mean feature...\n",
      "adding group max feature...\n",
      "adding group min feature...\n",
      "adding group size feature...\n",
      "adding match mean feature...\n",
      "adding match size feature...\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y_train = feature_engineering(train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_train).to_csv('X_train.csv', header=None)\n",
    "# pd.DataFrame(y_train).to_csv('y_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(X_train)\n",
    "# scaler = preprocessing.QuantileTransformer().fit(X_train)\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_MinMax.save']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# scaler_filename = \"scaler_MinMax.save\"\n",
    "# joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size):\n",
    "    input_X = Input((input_size, ))\n",
    "    X = Dense(32, activation='relu')(input_X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=input_X, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = get_model(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 247)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                7936      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 10,337\n",
      "Trainable params: 10,209\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.01, epsilon=1e-8, decay=1e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.compile(loss='mean_absolute_error',\n",
    "           optimizer=opt,\n",
    "           metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule, verbose)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.9, step_size=1, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"NN_3.model\", save_best_only=True, verbose=1)\n",
    "# reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1621395 samples, validate on 405349 samples\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
      "1621395/1621395 [==============================] - 58s 36us/step - loss: 0.0688 - mean_absolute_error: 0.0688 - val_loss: 0.0769 - val_mean_absolute_error: 0.0769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07685, saving model to NN_3.model\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.09000000000000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0531 - mean_absolute_error: 0.0531 - val_loss: 0.0875 - val_mean_absolute_error: 0.0875\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07685\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.08100000000000002.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0493 - mean_absolute_error: 0.0493 - val_loss: 0.0901 - val_mean_absolute_error: 0.0901\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07685\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0729.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0468 - mean_absolute_error: 0.0468 - val_loss: 0.0901 - val_mean_absolute_error: 0.0901\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07685\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.06561.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0456 - mean_absolute_error: 0.0456 - val_loss: 0.1031 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07685\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.05904900000000001.\n",
      "1621395/1621395 [==============================] - 10s 6us/step - loss: 0.0442 - mean_absolute_error: 0.0442 - val_loss: 0.1011 - val_mean_absolute_error: 0.1011\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07685\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.05314410000000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0433 - mean_absolute_error: 0.0433 - val_loss: 0.0989 - val_mean_absolute_error: 0.0989\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07685\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.04782969000000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0426 - mean_absolute_error: 0.0426 - val_loss: 0.1229 - val_mean_absolute_error: 0.1229\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07685\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.04304672100000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.1066 - val_mean_absolute_error: 0.1066\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07685\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.03874204890000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0412 - mean_absolute_error: 0.0412 - val_loss: 0.0992 - val_mean_absolute_error: 0.0992\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07685\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.03486784401000001.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0405 - mean_absolute_error: 0.0405 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07685\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.031381059609000006.\n",
      "1621395/1621395 [==============================] - 10s 6us/step - loss: 0.0402 - mean_absolute_error: 0.0402 - val_loss: 0.1207 - val_mean_absolute_error: 0.1207\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07685\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.028242953648100012.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0397 - mean_absolute_error: 0.0397 - val_loss: 0.1189 - val_mean_absolute_error: 0.1189\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07685\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.02541865828329001.\n",
      "1621395/1621395 [==============================] - 10s 6us/step - loss: 0.0395 - mean_absolute_error: 0.0395 - val_loss: 0.1115 - val_mean_absolute_error: 0.1115\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07685\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.02287679245496101.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0391 - mean_absolute_error: 0.0391 - val_loss: 0.1111 - val_mean_absolute_error: 0.1111\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07685\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.02058911320946491.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.1148 - val_mean_absolute_error: 0.1148\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07685\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.018530201888518418.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0386 - mean_absolute_error: 0.0386 - val_loss: 0.1000 - val_mean_absolute_error: 0.1000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.07685\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.016677181699666577.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.07685\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.015009463529699918.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0382 - mean_absolute_error: 0.0382 - val_loss: 0.0973 - val_mean_absolute_error: 0.0973\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07685\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.013508517176729929.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.07685\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.012157665459056936.\n",
      "1621395/1621395 [==============================] - 9s 6us/step - loss: 0.0379 - mean_absolute_error: 0.0379 - val_loss: 0.1061 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07685\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "train_history = m1.fit(X_train, y_train, batch_size=2048,\n",
    "                       epochs=50, validation_data=(X_dev, y_dev),\n",
    "                       callbacks=[early_stopping, model_checkpoint, lr_sched],\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test_V2.csv')\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new features...\n",
      "Removing Na's From DF\n",
      "False\n",
      "adding group mean feature...\n",
      "adding group max feature...\n",
      "adding group min feature...\n",
      "adding group size feature...\n",
      "adding match mean feature...\n",
      "adding match size feature...\n"
     ]
    }
   ],
   "source": [
    "X_test, _ = feature_engineering(X_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_test).to_csv('X_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "scaler = joblib.load('scaler_MinMax.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97727273, -1.        , -0.99050597, ..., -0.9986185 ,\n",
       "        -0.92129884,  0.83673469],\n",
       "       [-0.95454545, -0.6969697 , -0.88263301, ..., -0.99670277,\n",
       "        -0.94778481,  0.91836735],\n",
       "       [-0.93181818, -0.86363636, -0.8873942 , ..., -0.99770283,\n",
       "        -0.93186103,  0.87755102],\n",
       "       ...,\n",
       "       [-0.96969697, -0.93939394, -0.97226421, ..., -0.99786951,\n",
       "        -0.93778616,  0.87755102],\n",
       "       [-0.95454545, -0.81818182, -0.94358374, ..., -0.9972605 ,\n",
       "        -0.94488397,  0.91836735],\n",
       "       [-1.        , -1.        , -0.98838989, ..., -0.99770497,\n",
       "        -0.92432581,  0.83673469]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NN_2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999 199999 299999 399999 499999 599999 699999 799999 899999 999999 1099999 1199999 1299999 1399999 1499999 1599999 1699999 1799999 1899999 "
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    winPlacePerc = pred[i]\n",
    "    maxPlace = int(X_test.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "    \n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0    \n",
    "    pred[i] = winPlacePerc\n",
    "\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(i, flush=True, end=\" \")\n",
    "        \n",
    "\n",
    "X_test['winPlacePerc'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = X_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('submission_NN_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
